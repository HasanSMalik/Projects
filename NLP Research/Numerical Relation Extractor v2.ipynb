{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New revision of rule-based Numerical Relation Extractor.\n",
    "\n",
    "Changes w.r.t. previous (Thesis) extractor:\n",
    "\n",
    "Uses spaCy rather than quantulum to find quantities, and identifies values/units independently.\n",
    "\n",
    "Identifies certain linked quantities (e.g. from x to y) and can infer units for one of the quantities even if not explicit.\n",
    "\n",
    "Builds verb phrases.\n",
    "\n",
    "Sanity checks for entity phrase - improves precision.\n",
    "\n",
    "Entirely new code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from quantulum3 import parser\n",
    "import unicodedata\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from itertools import groupby\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds a token's parent noun positioned to the right of the token.\n",
    "# Used to compensate for spaCy not identifying a unit as part of the quantity phrase. \"500 points\"\n",
    "# Input: token\n",
    "# Output: parent noun\n",
    "def get_parent_noun(token):\n",
    "    try:\n",
    "        if token.head.i > token.i and (token.head.tag_[:2] == 'NN' or token.head.pos_ == 'NUM'):\n",
    "            token = token.head\n",
    "            return get_parent_noun(token)\n",
    "        else:\n",
    "            return token\n",
    "    except:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds a token's largest noun phrase (either from noun chunking or entity recognition).\n",
    "# Input: token\n",
    "# Output: token's noun chunk\n",
    "def get_chunk(token):\n",
    "    doc = token.doc\n",
    "    try:\n",
    "        tok_chunk = next(iter([chunk for chunk in doc.noun_chunks if token in chunk]), [])\n",
    "        tok_ent = next(iter([ent for ent in doc.ents if token in ent]), [])\n",
    "        if tok_ent and tok_ent[0].tag_ == \"IN\":\n",
    "            tok_ent = doc[tok_ent.start + 1:tok_ent.end]\n",
    "        if tok_chunk and len(tok_chunk)>len(tok_ent):\n",
    "            out_chunk = tok_chunk\n",
    "        elif tok_ent and len(tok_chunk)<=len(tok_ent):\n",
    "            out_chunk = tok_ent\n",
    "        else:\n",
    "            out_chunk = doc[token.i: token.i+1]\n",
    "        \n",
    "        if doc[out_chunk.end].text == 'per':\n",
    "            out_chunk = doc[out_chunk.start:get_chunk(doc[out_chunk.end + 1]).end]\n",
    "        \n",
    "        return out_chunk\n",
    "    except:\n",
    "        return doc[token.i: token.i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds the closest parent verb to a token in terms of dependency distance. \n",
    "# Input: token\n",
    "# Output: parent verb\n",
    "def get_parent_verb(token):\n",
    "    try:\n",
    "        if token == token.head:\n",
    "            return token\n",
    "        else:\n",
    "            token = token.head\n",
    "            if token.tag_[:2] not in ['VB','RB']: #token.pos_ != 'VERB' or \n",
    "                token = get_parent_verb(token)\n",
    "            return token\n",
    "    except:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructs a verb phrase by looking for consecutive verbs and adverbs. \n",
    "# Input: verb\n",
    "# Output: verb phrase\n",
    "\n",
    "def prev_verb(verb):\n",
    "    doc = verb.doc\n",
    "    if doc[verb.i-1].tag_[:2] in [\"VB\",\"RB\"]:\n",
    "        return prev_verb(doc[verb.i-1])\n",
    "    else:\n",
    "        return verb\n",
    "\n",
    "def next_verb(verb):\n",
    "    doc = verb.doc\n",
    "    if doc[verb.i+1].tag_[:2] in [\"VB\",\"RB\"]:\n",
    "        return next_verb(doc[verb.i+1])\n",
    "    else:\n",
    "        return verb\n",
    "\n",
    "def get_verb_chunk(verb):\n",
    "    doc = verb.doc\n",
    "    return doc[prev_verb(verb).i : next_verb(verb).i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds units before or after a quantity that spaCy may miss. \n",
    "# Identifies a quantity phrase starting with \"from\" and finds its second quantity by searching for \"to\".\n",
    "# Input: spaCy doc, quantity start/end indices, quantity phrase\n",
    "# Output: quantity phrase, second quantity (if exists)\n",
    "\n",
    "def get_missing_units(doc,q_ranges,q):\n",
    "    q2 = False\n",
    "    units_before = [child.i for child in q.root.subtree if child.i < q.start]\n",
    "    try:\n",
    "        out_phrase = doc[units_before[0]:get_parent_noun(q[-1]).i + 1]\n",
    "    except:\n",
    "        out_phrase = doc[q.start:get_parent_noun(q[-1]).i + 1]\n",
    "    \n",
    "    try:\n",
    "        if doc[out_phrase.start - 1].text == 'from':\n",
    "            temp = next(token for token in doc[out_phrase.start - 1].subtree if token.text == 'to')\n",
    "            q2 = next(q2 for q2 in doc[out_phrase.start - 1].subtree if (q2.i> temp.i) and (q2.tag_ == 'CD'))\n",
    "            temp_q2 = next(doc[tup[0]:tup[1]+1] for tup in q_ranges if tup[0] == q2.i)\n",
    "            temp_q2 = doc[temp_q2.start : get_chunk(temp_q2[-1]).end]\n",
    "            temp_q2 = doc[temp_q2.start:get_parent_noun(temp_q2[-1]).i + 1]\n",
    "            out_phrase = doc[out_phrase.start: temp_q2.end]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return out_phrase, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds a parent relation noun phrase to the left of the quantity. \n",
    "# Input: quantity phrase, verb\n",
    "# Output: list of noun phrases\n",
    "def get_ancestor_rel(quantity, verb):\n",
    "    tok_list = []\n",
    "    itter = 1\n",
    "    token = quantity.root\n",
    "    while token != verb and itter < len(token.doc):\n",
    "        if (token.head.i < quantity.start) and (token.head.tag_[:2] == 'NN'):\n",
    "            tok_list.append((get_chunk(token.head),(get_chunk(token.head).start,get_chunk(token.head).end)) )\n",
    "        token = token.head\n",
    "        itter += 1\n",
    "    return tok_list\n",
    "\n",
    "# Finds a relation noun phrase dependant on, and to the right of the quantity. \n",
    "# Input: quantity phrase\n",
    "# Output: list of noun phrases\n",
    "\n",
    "def get_descendant_rel(quantity):\n",
    "    token = quantity.root\n",
    "    return [(get_chunk(t),(get_chunk(t).start,get_chunk(t).end)) \\\n",
    "            for t in token.subtree if (t.i > quantity.end) and (t.tag_[:2] == 'NN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits a phrase consisting of two quantities into two phrases, and finds and compares their units.\n",
    "# Input: quantity phrase, values of both quantities\n",
    "# Output: units of both quantities\n",
    "def split_multi_q(q_phrase, value1, value2):\n",
    "    doc = q_phrase.doc\n",
    "    try:\n",
    "        split_ind = next(token.i for token in q_phrase if token.lower_ in ['and','or','to'])\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    q1_phrase = doc[q_phrase.start:split_ind]\n",
    "    q2_phrase = doc[split_ind + 1:q_phrase.end]\n",
    "    \n",
    "    \n",
    "    \" \".join([token.text for token in q1_phrase if not token.is_stop]).replace(value1.text,'').strip()\n",
    "    \n",
    "    q1_unit = \" \".join([token.text for token in q1_phrase if not token.is_stop or token.lower_ == 'per']).replace(value1.text,'').strip()\n",
    "            #\" \".join(q1_phrase.text.replace(value1.text,'').strip().split())\n",
    "    q2_unit = \" \".join([token.text for token in q2_phrase if not token.is_stop or token.lower_ == 'per']).replace(value2.text,'').strip()\n",
    "            #\" \".join(q2_phrase.text.replace(value2.text,'').strip().split())\n",
    "        \n",
    "    if not q1_unit:\n",
    "        out_unit_1 = q2_unit\n",
    "        out_unit_2 = q2_unit\n",
    "    \n",
    "    elif not q2_unit:\n",
    "        out_unit_1 = q1_unit\n",
    "        out_unit_2 = q1_unit\n",
    "    \n",
    "    else:\n",
    "        out_unit_1 = q1_unit\n",
    "        out_unit_2 = q2_unit\n",
    "    \n",
    "    return out_unit_1, out_unit_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the numerical relation extraction tuple given a quantity phrase\n",
    "# Input: doc, quantity phrase, number of quantities in the phrase, indices of the numbers, indices of the quantities\n",
    "# Output: extraction tuple for the quantity phrase\n",
    "\n",
    "def get_extraction(doc, q_phrase, rev_q_phrases_dict, q_phrases_list, q_ranges):\n",
    "\n",
    "    # IDENTIFYING VERB\n",
    "    \n",
    "    verb = get_parent_verb(q_phrase.root)\n",
    "    \n",
    "    if verb.tag_[:2] == 'RB':\n",
    "        out_verb = get_verb_chunk(verb)\n",
    "        verb = get_parent_verb(verb)\n",
    "    elif verb.tag_[:2] == 'VB':\n",
    "        out_verb = doc[verb.i : verb.i + 1]\n",
    "    else:\n",
    "        raise ValueError(\"Quantity has no parent verb.\")\n",
    "    \n",
    "    #print('verb is', verb)\n",
    "    \n",
    "    # FINDING RELATION WORDS\n",
    "    \n",
    "    rel_A_list = get_ancestor_rel(q_phrase, verb)\n",
    "    rel_D_list = get_descendant_rel(q_phrase)\n",
    "    \n",
    "    try:\n",
    "        a = [bool(set(q_phrases_list).intersection(range(*tup[1]))) for tup in rel_A_list].index(False)\n",
    "    except:\n",
    "        a = 10\n",
    "    try:\n",
    "        b = [bool(set(q_phrases_list).intersection(range(*tup[1]))) for tup in rel_D_list].index(False)\n",
    "    except:\n",
    "        b = 10\n",
    "    \n",
    "    if a < b:\n",
    "        out_relation = rel_A_list[a][0]\n",
    "    elif b < a:\n",
    "        out_relation = rel_D_list[b][0]\n",
    "    elif (a == b) and (a != 10):\n",
    "        out_relation = rel_A_list[a][0]\n",
    "    else:\n",
    "        out_relation = ''\n",
    "    \n",
    "    #print('relation is', out_relation)\n",
    "    \n",
    "    # FINDING ENTITY\n",
    "    \n",
    "    try:\n",
    "        nsubj = next(child for child in verb.subtree if (child.dep_ in ['nsubj','nsubjpass']) and child.i < verb.i)\n",
    "    except:\n",
    "        verb = get_parent_verb(verb)\n",
    "        nsubj = next(child for child in verb.subtree \\\n",
    "                     if (child.dep_ in ['nsubj','nsubjpass']) and child.i < verb.i)\n",
    "    \n",
    "    if (get_verb_chunk(verb).start > 0) and doc[get_verb_chunk(verb).start - 1].dep_ == 'punct':\n",
    "        out_entity = get_chunk(nsubj)\n",
    "    else:\n",
    "        try:\n",
    "            out_entity = [get_chunk(child) for child in verb.subtree \\\n",
    "                          if (child.tag_[:2]=='NN' or child.dep_ in ['nsubj','nsubjpass'])\\\n",
    "                          and (child.i < verb.i)][-1]\n",
    "            if out_relation == '' and out_entity.root.dep_ not in ['nsubj','nsubjpass']:\n",
    "                out_relation = get_chunk(nsubj)\n",
    "        except:\n",
    "            out_entity = get_chunk(nsubj)\n",
    "\n",
    "    #print('entity is', out_entity)\n",
    "    \n",
    "    # HANDLING QUANTITIES\n",
    "    \n",
    "    if len(rev_q_phrases_dict[q_phrase]) == 1:\n",
    "        out_value_1 = doc[rev_q_phrases_dict[q_phrase][0][0]: rev_q_phrases_dict[q_phrase][0][1]+1]\n",
    "        out_unit_1 = \" \".join([token.text for token in q_phrase if (not token.is_stop) or token.lower_ == 'per']).replace(out_value_1.text,'').strip()\n",
    "                    #\" \".join(q_phrase.text.replace(out_value_1.text,'').strip().split())\n",
    "        out_value_2, out_unit_2 = '', ''\n",
    "        try:\n",
    "            # use quantulum to standardize value\n",
    "            out_value_1 = parser.parse(out_value_1.text)[0].value\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    elif len(rev_q_phrases_dict[q_phrase]) == 2:\n",
    "        out_value_1 = doc[rev_q_phrases_dict[q_phrase][0][0]: rev_q_phrases_dict[q_phrase][0][1]+1]\n",
    "        out_value_2 = doc[rev_q_phrases_dict[q_phrase][1][0]: rev_q_phrases_dict[q_phrase][1][1]+1]\n",
    "        out_unit_1, out_unit_2 = split_multi_q(q_phrase, out_value_1, out_value_2)\n",
    "        \n",
    "        try:\n",
    "            # use quantulum to standardize values\n",
    "            out_value_1 = parser.parse(out_value_1.text)[0].value\n",
    "            out_value_2 = parser.parse(out_value_2.text)[0].value\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "\n",
    "    # QUANTULUM OUTPUTS\n",
    "    \n",
    "    quants = parser.parse(q_phrase.text)\n",
    "    try:\n",
    "        quantulum_v1, quantulum_u1, quantulum_e1 = quants[0].value, quants[0].unit.name, quants[0].unit.entity.name\n",
    "    except:\n",
    "        quantulum_v1, quantulum_u1, quantulum_e1 = '', '', ''\n",
    "    try:\n",
    "        quantulum_v2, quantulum_u2, quantulum_e2 = quants[1].value, quants[1].unit.name, quants[1].unit.entity.name\n",
    "    except:\n",
    "        quantulum_v2, quantulum_u2, quantulum_e2 = '', '', ''\n",
    "    \n",
    "    # check if entity occurs in any other output, raise error if it is.\n",
    "    if out_entity and out_verb and (out_entity.start in \\\n",
    "                                                 [*range(out_verb.start,out_verb.end),\\\n",
    "                                                  *range(q_phrase.start,q_phrase.end)]):\n",
    "        raise ValueError(\"Entity is not distinct.\")\n",
    "    elif str(out_entity) in [str(out_verb), str(out_relation), str(q_phrase)]:\n",
    "        raise ValueError(\"Entity is not distinct.\")\n",
    "    \n",
    "    \n",
    "    # form output\n",
    "    try:\n",
    "        arg1_f = \" \".join([token.text for token in nlp(str(out_entity)) if not token.is_stop])\n",
    "    except:\n",
    "        arg1_f = out_entity\n",
    "    try:\n",
    "        rel_f = \" \".join([token.text for token in nlp(str(out_relation)) if not token.is_stop])\n",
    "    except:\n",
    "        rel_f = out_relation\n",
    "    \n",
    "    out_extr = [doc, out_entity, out_verb, out_relation, q_phrase, out_value_1, out_unit_1,\\\n",
    "                out_value_2, out_unit_2, \\\n",
    "                quantulum_e1, quantulum_v1, quantulum_u1, quantulum_e2, quantulum_v2, quantulum_u2, arg1_f, rel_f]\n",
    "    \n",
    "    out_extr = [str(x) for x in out_extr]\n",
    "    \n",
    "    \n",
    "    return out_extr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops over a text to identify quantities and calls the extraction function for each quantity phrase.\n",
    "# Input: text\n",
    "# Output: list of extractions for each quantity phrase in the text.\n",
    "\n",
    "def get_NRE(txt):\n",
    "\n",
    "    txt = unicodedata.normalize('NFKD',txt)\n",
    "    doc = nlp(' '.join(txt.split()))\n",
    "    #print(doc)\n",
    "    \n",
    "    q_phrases_list = [token.i for token in doc \\\n",
    "                  if token.ent_type_ in ['TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'CARDINAL', 'DATE'] \\\n",
    "                  and token.tag_ == \"CD\" and not (token.ent_type_ == 'DATE' and token.shape_ == 'dddd')]\n",
    "    #print(q_phrases_list)\n",
    "    \n",
    "    q_ranges =[]\n",
    "    for k,g in groupby(enumerate(q_phrases_list),lambda x:x[0]-x[1]):\n",
    "        group = (map(itemgetter(1),g))\n",
    "        group = list(map(int,group))\n",
    "        q_ranges.append((group[0],group[-1]))\n",
    "    #print(q_ranges)\n",
    "\n",
    "    q_phrases_dict = {}\n",
    "    for i in q_ranges:\n",
    "        quantity = doc[i[0]:i[1] +1]\n",
    "        quantity_chunk = doc[get_chunk(quantity[0]).start : get_chunk(quantity[-1]).end]\n",
    "        q_phrases_dict[i] = quantity_chunk\n",
    "    #print(q_phrases_dict)\n",
    "\n",
    "    temp_dict = {}\n",
    "    for key, value in q_phrases_dict.items():\n",
    "        phrase, q2 = get_missing_units(doc,q_ranges,value)\n",
    "        if q2:\n",
    "            temp_dict[key] = phrase\n",
    "            for k in q_phrases_dict.keys():\n",
    "                if k[0] == q2.i:\n",
    "                    temp_dict[k] = phrase\n",
    "        elif key in temp_dict:\n",
    "                pass\n",
    "        else:\n",
    "            temp_dict[key] = phrase\n",
    "    #print(temp_dict)\n",
    "\n",
    "    rev_q_phrases_dict = {}\n",
    "    for key, value in temp_dict.items():\n",
    "        rev_q_phrases_dict.setdefault(value, list()).append(key)\n",
    "    \n",
    "    #print(rev_q_phrases_dict)\n",
    "\n",
    "    out_extract_list = []\n",
    "    for q_phrase in rev_q_phrases_dict.keys():\n",
    "        try:\n",
    "            #print(q_phrase)\n",
    "            extraction = get_extraction(doc, q_phrase, rev_q_phrases_dict, q_phrases_list, q_ranges)\n",
    "            out_extract_list.append(extraction)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return out_extract_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing row  2001  to row  2101\n",
      "processing row  2101  to row  2201\n",
      "processing row  2201  to row  2301\n",
      "processing row  2301  to row  2401\n",
      "processing row  2401  to row  2501\n",
      "processing row  2501  to row  2601\n",
      "processing row  2601  to row  2701\n",
      "processing row  2701  to row  2801\n",
      "processing row  2801  to row  2901\n",
      "processing row  2901  to row  3001\n",
      "processing row  3001  to row  3101\n",
      "processing row  3101  to row  3201\n",
      "processing row  3201  to row  3301\n",
      "processing row  3301  to row  3401\n",
      "processing row  3401  to row  3501\n",
      "processing row  3501  to row  3601\n",
      "processing row  3601  to row  3701\n",
      "processing row  3701  to row  3801\n",
      "processing row  3801  to row  3901\n",
      "processing row  3901  to row  4001\n"
     ]
    }
   ],
   "source": [
    "f_out = open(\"finance_NewExtractions.csv\", mode=\"a\", newline='', encoding=\"utf-8\")\n",
    "extract_writer = csv.writer(f_out, delimiter=',',quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "# process corpus 100 documents at a time into pandas dataframe.\n",
    "for row in range(2001,4001,100):\n",
    "    df = pd.read_csv(\"articles.csv\", usecols=[1,3,4], nrows=100, encoding=\"utf-8\", skiprows=range(1, row))\n",
    "    df = df.dropna().reset_index(drop=True) # drop empty rows\n",
    "    df = df[df.body.str.contains('\\d+')].reset_index(drop=True) # drop any documents not containing any numbers\n",
    "    df.body = df.body.map(lambda txt: ' '.join(txt.split())) # remove extra whitespaces from each document\n",
    "    print('processing row ',row,' to row ', row+100)\n",
    "    \n",
    "    # loop extraction over each document in corpus\n",
    "    for txt in df.body:\n",
    "        txt = unicodedata.normalize('NFKD',txt)\n",
    "        doc = nlp(txt)\n",
    "        \n",
    "        # document-level coreference resolution\n",
    "        # txt = doc._.coref_resolved\n",
    "        # doc = nlp(txt)\n",
    "        # split document into sentences\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        for sent in sentences:\n",
    "            try:\n",
    "                extractions = get_NRE(str(sent))\n",
    "                for e in extractions:\n",
    "                    extract_writer.writerow(e)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5001\n",
      "10001\n",
      "15001\n",
      "20001\n",
      "25001\n",
      "30001\n",
      "35001\n",
      "40001\n",
      "45001\n",
      "50001\n",
      "55001\n"
     ]
    }
   ],
   "source": [
    "f_in = open(\"astrophysics_corpus.txt\", \"r\", newline='', encoding=\"utf-8\")\n",
    "f_out = open(\"astro_NewExtractions.csv\", mode=\"w\", newline='', encoding=\"utf-8\")\n",
    "extract_writer = csv.writer(f_out, delimiter=',',quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "rownum = 1\n",
    "for row in f_in:\n",
    "    \n",
    "    if rownum in range(1,200002, 5000):\n",
    "        print(rownum)\n",
    "    \n",
    "    if rownum < 200002:\n",
    "        try:\n",
    "            extractions = get_NRE(str(row))\n",
    "            for e in extractions:\n",
    "                extract_writer.writerow(e)\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    rownum +=1\n",
    "\n",
    "f_in.close()\n",
    "f_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
