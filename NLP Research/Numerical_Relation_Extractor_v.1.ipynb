{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule based Open Numerical Relation Extractor v1 (as described in Thesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from quantulum3 import parser\n",
    "#import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import unicodedata\n",
    "#import neuralcoref\n",
    "#neuralcoref.add_to_pipe(nlp)\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for forming extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns noun chunk of a passed token\n",
    "def get_chunk(token):\n",
    "    try:\n",
    "        return [chunk for chunk in doc.noun_chunks if token in chunk][0]\n",
    "    except:\n",
    "        return doc[token.i: token.i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns noun chunk of a passed phrase\n",
    "def get_chunk2(text):\n",
    "    try:\n",
    "        return doc[[chunk for chunk in doc.noun_chunks if text[0] in chunk][0].start : text.end]\n",
    "\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns parent verb of a passed token\n",
    "def get_parent_verb(token):\n",
    "    try:\n",
    "        token = token.head\n",
    "        if token.pos_ != 'VERB':\n",
    "            token = get_parent_verb(token)\n",
    "        return token\n",
    "    except:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns start and end token indicies of quantities in a doc\n",
    "def get_indices(doc, quants):\n",
    "\n",
    "    quant_starts = []\n",
    "    quant_ends = []\n",
    "    \n",
    "    for token in doc:\n",
    "        for quant in quants:\n",
    "            if token.idx == quant.span[0]:\n",
    "                quant_starts.append(token.i)\n",
    "            if token.idx == quant.span[1] or token.idx == quant.span[1] + 1:\n",
    "                quant_ends.append(token.i)\n",
    "                \n",
    "    qSpans = list(zip(quant_starts,quant_ends))\n",
    "    \n",
    "    return qSpans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns joined consecutive noun phrases\n",
    "def combine_phrase(ind, prev_or_next,noun_list):\n",
    "    if prev_or_next == 'prev':\n",
    "        if noun_list[ind-1][0].pos_ in ['NOUN','PROPN']: \n",
    "            return combine_phrase(ind-1,'prev',noun_list)\n",
    "        else:\n",
    "            return (noun_list[ind][2],ind)\n",
    "    \n",
    "    if prev_or_next == 'next':\n",
    "        \n",
    "        try:\n",
    "            if noun_list[ind+1][0].pos_ in ['NOUN','PROPN']: \n",
    "                try:\n",
    "                    return combine_phrase(ind+1,'next',noun_list)\n",
    "                except:\n",
    "                    return (noun_list[ind+1][2],ind+1)\n",
    "            else:\n",
    "                return (noun_list[ind][2],ind)\n",
    "        except:\n",
    "            return (noun_list[ind][2],ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function and keywords used to extract in BONIE test data\n",
    "# returns extraction given a document and keywords.\n",
    "\n",
    "keywords = {\n",
    "    'length':{\n",
    "        'height':['height','high','highest', 'elevation', 'altitude', 'tall'],\n",
    "        'length':['length','long','longest'],\n",
    "        'width':['width','wide','breadth'],\n",
    "        'distance':['distance','away','furthest'],\n",
    "        'depth':['depth','deep']\n",
    "    },\n",
    "    'mass':{\n",
    "        'weight':['weight','weighs'],\n",
    "        'mass':['mass']\n",
    "    }}\n",
    "\n",
    "def extract(q,quants,qSpans,doc,keywords):\n",
    "    qroot = get_chunk(doc[qSpans[q][0]:qSpans[q][1]].root).root\n",
    "    verb = get_parent_verb(doc[qSpans[q][0]])\n",
    "\n",
    "    if not [t for t in verb.lefts if t.pos_ in ['NOUN','PROPN','PRON']]:\n",
    "        verb = get_parent_verb(get_parent_verb(doc[qSpans[q][0]]))\n",
    "\n",
    "    # create a list\n",
    "    noun_list = [(token,token.dep_,token.i) for token in verb.subtree \n",
    "                 if (token.dep_ in ['poss','pobj','dobj','attr','nsubj','nsubjpass','punct','det','cc']\n",
    "                     or token in [qroot,verb]) and token.dep_ != 'compound']\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # remove subphrases\n",
    "    sub_phrase = [(token,token.dep_,token.i) for token in verb.subtree \n",
    "         if token.text == ',' or token in [qroot,verb] or (token.head == verb and token.dep_ in ['nsubj','nsubjpass'])]\n",
    "\n",
    "    for i in range(len(sub_phrase) -1):\n",
    "        if sub_phrase[i][1] == 'punct' and sub_phrase[i+1][1] == 'punct':\n",
    "            del noun_list[noun_list.index(sub_phrase[i]):noun_list.index(sub_phrase[i+1])+1]\n",
    "    try:\n",
    "        if sub_phrase[0][1] == 'punct':\n",
    "            del noun_list[:noun_list.index(sub_phrase[0])]\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        if sub_phrase[-1][1] == 'punct':\n",
    "            del noun_list[noun_list.index(sub_phrase[-1]):]\n",
    "    except:\n",
    "        pass\n",
    "    #--------------------------------------------------------------\n",
    "    q_ind = noun_list.index((qroot, qroot.dep_,qroot.i))\n",
    "    v_ind = noun_list.index((verb, verb.dep_,verb.i))\n",
    "    kword = []\n",
    "    kword_fixed = []\n",
    "\n",
    "    \n",
    "    if qroot.dep_ in ['nsubj','nsubjpass']:\n",
    "\n",
    "        try:\n",
    "            p_ind = noun_list.index([i for i in noun_list[v_ind:] if i[1] == 'punct'][0])\n",
    "        except:\n",
    "            p_ind = len(doc)-1\n",
    "        #-------------------------------------------------------------------------------------#\n",
    "        # split noun chunks containing a possessive to get entity's keyword\n",
    "        #-------------------------------------------------------------------------------------#\n",
    "        if noun_list[q_ind+1][0].pos_ in ['DET','PROPN','NOUN']:\n",
    "            ent_ind = q_ind+1\n",
    "            entity = doc[noun_list[ent_ind][2]:combine_phrase(ent_ind,'next',noun_list)[0] + 1]\n",
    "            kword.append(doc[verb.i + 1 :noun_list[p_ind][2]])\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                ent_ind = noun_list.index([i for i in noun_list[v_ind:p_ind] if i[1] == 'poss'][0])\n",
    "                entity = doc[combine_phrase(ent_ind,'prev',noun_list)[0] :noun_list[ent_ind][2] + 1]\n",
    "\n",
    "                kword_fixed.append(doc[noun_list[ent_ind+1][2] : combine_phrase(ent_ind+1,'next',noun_list)[0]+1])\n",
    "            except:\n",
    "            #-------------------------------------------------------------------------------------#\n",
    "            # If > 1 determiner before verb, first is relation, second is entity\n",
    "            #-------------------------------------------------------------------------------------#\n",
    "                try:\n",
    "                    ent_ind = noun_list.index([i for i in noun_list[v_ind:p_ind] if i[1] == 'det'][1])\n",
    "                except:\n",
    "                    try:\n",
    "                        ent_ind = noun_list.index([i for i in noun_list[v_ind:p_ind] if i[0].pos_ == 'PROPN' and i[1] == 'pobj'][0])\n",
    "                    except:\n",
    "                        try: \n",
    "                            ent_ind = noun_list.index([i for i in noun_list[v_ind:p_ind] if i[1] == 'pobj'][0])\n",
    "                        except:\n",
    "                            ent_ind = noun_list.index([i for i in noun_list[v_ind:p_ind] if i[0].pos_ == 'NOUN'][0])  ### added this\n",
    "\n",
    "                entity = doc[noun_list[ent_ind][2]:combine_phrase(ent_ind,'next',noun_list)[0] + 1]\n",
    "\n",
    "            ent_ind = noun_list.index((entity[0],entity[0].dep_,entity.start))\n",
    "\n",
    "            temp = [i for i in noun_list[v_ind + 1 :ent_ind] if i[0].pos_ == 'NOUN']\n",
    "            try:\n",
    "                kword.append(doc[get_chunk(doc[temp[0][2]]).start:temp[-1][2]+1])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            p_ind = noun_list.index([i for i in noun_list[:v_ind] if i[1] == 'punct'][-1])\n",
    "        except:\n",
    "            p_ind = 0\n",
    "        #-------------------------------------------------------------------------------------#\n",
    "        # split noun chunks containing a possessive to get entity's keyword\n",
    "        #-------------------------------------------------------------------------------------#\n",
    "        try:\n",
    "            ent_ind = noun_list.index([i for i in noun_list[p_ind:v_ind] if i[1] == 'poss' and doc[i[2]+1].tag_ == 'POS'][-1])\n",
    "            kword_fixed.append(doc[noun_list[ent_ind+1][2] : combine_phrase(ent_ind+1,'next',noun_list)[0]+1])\n",
    "        except:\n",
    "            ent_ind = v_ind-1\n",
    "\n",
    "        #-------------------------------------------------------------------------------------#\n",
    "        # If > 1 determiner before verb, first is relation, second is entity\n",
    "        #-------------------------------------------------------------------------------------#\n",
    "        if len([i for i in noun_list[p_ind:ent_ind] if i[1] == 'det'])>1:\n",
    "            entity = doc[combine_phrase(ent_ind,'prev',noun_list)[0] :noun_list[ent_ind][2] + 1]\n",
    "        else:         \n",
    "            try:\n",
    "                entity = doc[[i for i in noun_list[p_ind:ent_ind] if i[0].pos_ == 'PROPN'][0][2]:noun_list[ent_ind][2] + 1]\n",
    "            except:\n",
    "                entity = doc[noun_list[ent_ind][2]:noun_list[ent_ind][2]+1]\n",
    "        ent_ind = noun_list.index((entity[0],entity[0].dep_,entity.start)) \n",
    "\n",
    "        #------------------------#\n",
    "        temp = [i for i in noun_list[p_ind:ent_ind] if i[0].pos_ == 'NOUN']\n",
    "        try:\n",
    "            kword.append(doc[get_chunk(doc[temp[0][2]]).start:temp[-1][2]+1])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # any nouns before or after the quantity are keywords\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "\n",
    "        try: \n",
    "            if noun_list[q_ind - 1][1] in ['pobj','dobj']:\n",
    "                kword.append(doc[combine_phrase(q_ind-1,'prev',noun_list)[0] :noun_list[q_ind-1][2] + 1])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "        try: \n",
    "            if noun_list[q_ind +1][1] in ['pobj','dobj','det']:\n",
    "                kword.append(doc[noun_list[q_ind][2] +1 :combine_phrase(q_ind+1,'next',noun_list)[0] + 1])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # want to output quantity's parent verb if different from entity's\n",
    "    verb = get_parent_verb(doc[qSpans[q][0]])\n",
    "\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    # handling change words, building quantity output using units from quantulum if present\n",
    "    #-------------------------------------------------------------------------------------#\n",
    "    change_w = ['incline','grow','increase','surge','rise','climb','decline','decrease','fall','tumble','decline','crash']\n",
    "\n",
    "    if quants[q].unit.entity.name != 'dimensionless' and verb.lemma_ not in change_w:\n",
    "        verb_out = 'is'\n",
    "    else:\n",
    "        verb_out = verb.text\n",
    "\n",
    "    if doc[get_chunk(qroot).start - 1].pos_ == 'ADP' and verb_out != 'is':\n",
    "        verb_out = verb_out +' '+ doc[get_chunk(qroot).start - 1].text\n",
    "\n",
    "    try:\n",
    "        if doc[qroot.i + 1].pos_ == 'ADJ':\n",
    "            kword.append(doc[qroot.i+1:qroot.i+2])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if quants[q].unit.name != 'dimensionless':\n",
    "\n",
    "\n",
    "        quantity = doc[qSpans[q][0]:[word.i for word in doc[qSpans[q][0]:qroot.i + 1] if word.text in quants[q].surface][-1] + 1]\n",
    "        quant_out = quantity.text.replace(quants[q].surface ,str(quants[q].value) + ' ' + quants[q].unit.name)\n",
    "        ###################\n",
    "        quant_value, quant_unit = quants[q].value, quants[q].unit.name\n",
    "        ###################\n",
    "        try:\n",
    "            kword_fixed.append(doc[quantity.end:qroot.i + 1])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    else:\n",
    "        quant_out = doc[qSpans[q][0]:qroot.i + 1].text\n",
    "        quant_value, quant_unit = quants[q].value, doc[qSpans[q][0]:qroot.i + 1].root\n",
    "\n",
    "    entity_out = doc[get_chunk(entity[0]).start : entity.end]\n",
    "\n",
    "    #------------------------------------------------\n",
    "    #Building relation using keywords + unit\n",
    "    #------------------------------------------------\n",
    "\n",
    "    try:\n",
    "        kword_fixed = [doc[[word.i for word in get_chunk2(kword_fixed[0]) if word.dep_ == 'case'][-1]+1:get_chunk2(kword_fixed[0]).end]]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    rel_list = kword_fixed + [get_chunk2(text) for text in kword]\n",
    "\n",
    "    templist = []\n",
    "    for chunk in rel_list:\n",
    "        if doc[chunk.start-1].pos_ == 'ADP':\n",
    "            templist.append(doc[chunk.start - 1:chunk.start])\n",
    "        templist.append(chunk)\n",
    "\n",
    "\n",
    "    rel_list = [word for chunk in templist for word in chunk if word.pos_ != 'DET']\n",
    "\n",
    "    try:\n",
    "        rel_unit = [key for key, value in keywords[quants[q].unit.entity.name].items() for w in rel_list \n",
    "                    for v in value if v in w.lower_][0] + ' '\n",
    "\n",
    "    except:\n",
    "        rel_unit = ''\n",
    "        #if quants[q].unit.entity.name not in ['dimensionless','currency','unknown']:\n",
    "        #    rel_unit = quants[q].unit.entity.name + ' '\n",
    "        #else:\n",
    "        #    rel_unit = ''\n",
    "\n",
    "    if rel_list and rel_list[-1].tag_ == 'JJR':\n",
    "        verb_out = str(verb_out) + ' ' + rel_list[-1].text\n",
    "\n",
    "    verb_out = str(verb_out)\n",
    "\n",
    "    try:\n",
    "        if rel_list[-1].text in keywords[quants[q].unit.entity.name][rel_unit.strip()] and rel_list[-1].pos_ == 'ADJ':\n",
    "            rel_list = rel_list[:-1]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if not rel_list:\n",
    "        relation = rel_unit.strip()\n",
    "\n",
    "    elif rel_unit.strip() not in \" \".join(map(str,rel_list)):\n",
    "        if rel_list[0].pos_ == 'ADP':\n",
    "            relation = rel_unit + \" \".join(map(str,rel_list))\n",
    "        else:\n",
    "            relation = rel_unit + 'of '+ \" \".join(map(str,rel_list))\n",
    "    else:\n",
    "        if rel_list[0].pos_ == 'ADP' and rel_list[0].lower_ != 'per':\n",
    "            relation = \" \".join(map(str,rel_list[1:]))\n",
    "        else:\n",
    "            relation = \" \".join(map(str,rel_list))\n",
    "    #-----------------------------------    \n",
    "    return [doc.text, entity_out, relation, verb_out, quant_out, quant_value, quant_unit, quants[q].unit.entity.name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data and form extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell outputs extractions in the form (entity, relation, verb, quantity, q_value, q_unit) to a csv file.\n",
    "# for txt files\n",
    "\n",
    "f_in = open(\"astrophysics_corpus.txt\", \"r\", newline='', encoding=\"utf-8\")\n",
    "f_out = open(\"astro_MyExtr.csv\", mode=\"w\", newline='', encoding=\"utf-8\")\n",
    "extract_writer = csv.writer(f_out, delimiter=',',quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "for row in f_in:\n",
    "    \n",
    "    txt = unicodedata.normalize('NFKD',str(row))\n",
    "    doc = nlp(txt)\n",
    "\n",
    "    # document-level coreference resolution\n",
    "    #txt = doc._.coref_resolved\n",
    "    #doc = nlp(txt)\n",
    "    \n",
    "    # split document into sentences\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    for sent in sentences:\n",
    "        try:\n",
    "            txt = unicodedata.normalize('NFKD',sent)\n",
    "            doc = nlp(txt)\n",
    "\n",
    "            # use quantulum parser to identify quantities we wish to extract \n",
    "            quants = parser.parse(doc.text)\n",
    "\n",
    "            # remove long-form numbers and dates from list of quantities to extract\n",
    "            quants = [quant for quant in quants if any(char.isdigit() for char in quant.surface)]\n",
    "            q_dates = [quant for token in doc for quant in quants if (token.idx == quant.span[0]) and (token.ent_type_ == 'DATE')]\n",
    "            quants = [x for x in quants if x not in q_dates]\n",
    "\n",
    "            # get token index of quantities from their string character index\n",
    "            qSpans = get_indices(doc, quants)\n",
    "\n",
    "\n",
    "            # output extraction for each detected quantity \n",
    "            for q in range(len(qSpans)):\n",
    "                try:\n",
    "                    a = extract(q, quants, qSpans,doc,keywords)\n",
    "                    extract_writer.writerow(a)\n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "f_in.close()\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing row  101  to row  201\n",
      "processing row  201  to row  301\n",
      "processing row  301  to row  401\n",
      "processing row  401  to row  501\n",
      "processing row  501  to row  601\n",
      "processing row  601  to row  701\n",
      "processing row  701  to row  801\n",
      "processing row  801  to row  901\n",
      "processing row  901  to row  1001\n",
      "processing row  1001  to row  1101\n",
      "processing row  1101  to row  1201\n",
      "processing row  1201  to row  1301\n",
      "processing row  1301  to row  1401\n",
      "processing row  1401  to row  1501\n"
     ]
    }
   ],
   "source": [
    "# Cell outputs extractions in the form (entity, relation, verb, quantity, q_value, q_unit) to a csv file.\n",
    "# for dataframes.\n",
    "\n",
    "f_out = open(\"extractions.csv\", mode=\"w\", newline='', encoding=\"utf-8\")\n",
    "extract_writer = csv.writer(f_out, delimiter=',',quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "# process corpus 100 documents at a time into pandas dataframe.\n",
    "for row in range(101,1500,100):\n",
    "    df = pd.read_csv(\"articles.csv\", usecols=[1,3,4], nrows=100, encoding=\"utf-8\", skiprows=range(1, row))\n",
    "    df = df.dropna().reset_index(drop=True) # drop empty rows\n",
    "    df = df[df.body.str.contains('\\d+')].reset_index(drop=True) # drop any documents not containing any numbers\n",
    "    df.body = df.body.map(lambda txt: ' '.join(txt.split())) # remove extra whitespaces from each document\n",
    "    print('processing row ',row,' to row ', row+100)\n",
    "    \n",
    "    # loop extraction over each document in corpus\n",
    "    for txt in df.body:\n",
    "        txt = unicodedata.normalize('NFKD',txt)\n",
    "        doc = nlp(txt)\n",
    "        \n",
    "        # document-level coreference resolution\n",
    "        txt = doc._.coref_resolved\n",
    "        doc = nlp(txt)\n",
    "        # split document into sentences\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        for sent in sentences:\n",
    "            txt = unicodedata.normalize('NFKD',sent)\n",
    "            doc = nlp(txt)\n",
    "            \n",
    "            # use quantulum parser to identify quantities we wish to extract \n",
    "            quants = parser.parse(doc.text)\n",
    "            \n",
    "            # remove long-form numbers and dates from list of quantities to extract\n",
    "            quants = [quant for quant in quants if any(char.isdigit() for char in quant.surface)]\n",
    "            q_dates = [quant for token in doc for quant in quants if (token.idx == quant.span[0]) and (token.ent_type_ == 'DATE')]\n",
    "            quants = [x for x in quants if x not in q_dates]\n",
    "            \n",
    "            # get token index of quantities from their string character index\n",
    "            qSpans = get_indices(doc, quants)\n",
    "\n",
    "\n",
    "            # output extraction for each detected quantity \n",
    "            for q in range(len(qSpans)):\n",
    "                try:\n",
    "                    a = extract(q, quants, qSpans,doc,keywords)\n",
    "                    extract_writer.writerow(a)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
